/**
 * Unified docs API handler for @farming-labs/theme.
 *
 * A single route handler that serves **both** search and AI chat:
 *
 * - `GET  /api/docs?query=…` → full-text search over indexed MDX pages
 * - `POST /api/docs` → RAG-powered "Ask AI" (searches relevant docs,
 *   then streams an LLM response using the docs as context)
 *
 * This replaces the old `createDocsSearchAPI` — one handler, one route.
 *
 * @example
 * ```ts
 * // app/api/docs/route.ts (auto-generated by withDocs)
 * import { createDocsAPI } from "@farming-labs/theme/api";
 * export const { GET, POST } = createDocsAPI();
 * export const revalidate = false;
 * ```
 */

import fs from "node:fs";
import path from "node:path";
import matter from "gray-matter";
import { createSearchAPI } from "fumadocs-core/search/server";

// ─── Types ──────────────────────────────────────────────────────────

interface SearchIndex {
  title: string;
  description?: string;
  content: string;
  url: string;
}

interface AIProviderConfig {
  baseUrl: string;
  apiKey?: string;
}

interface AIModelEntry {
  id: string;
  label: string;
  provider?: string;
}

interface AIModelConfig {
  models?: AIModelEntry[];
  defaultModel?: string;
}

interface AIOptions {
  enabled?: boolean;
  model?: string | AIModelConfig;
  providers?: Record<string, AIProviderConfig>;
  systemPrompt?: string;
  /** Default baseUrl when no per-model provider is configured. */
  baseUrl?: string;
  /** Default apiKey when no per-model provider is configured. */
  apiKey?: string;
  maxResults?: number;
}

interface DocsAPIOptions {
  /** Docs entry folder (default: read from docs.config) */
  entry?: string;
  /** Search language (default: "english") */
  language?: string;
  /** AI chat configuration */
  ai?: AIOptions;
}

// ─── Helpers ────────────────────────────────────────────────────────

const FILE_EXTS = ["tsx", "ts", "jsx", "js"];

function readEntry(root: string): string {
  for (const ext of FILE_EXTS) {
    const configPath = path.join(root, `docs.config.${ext}`);
    if (fs.existsSync(configPath)) {
      try {
        const content = fs.readFileSync(configPath, "utf-8");
        const match = content.match(/entry\s*:\s*["']([^"']+)["']/);
        if (match) return match[1];
      } catch {
        // fall through
      }
    }
  }
  return "docs";
}

/**
 * Read AI config from docs.config by parsing the file for the `ai` block.
 * This avoids importing the config (which may use JSX/React).
 */
function readAIConfig(root: string): AIOptions {
  for (const ext of FILE_EXTS) {
    const configPath = path.join(root, `docs.config.${ext}`);
    if (fs.existsSync(configPath)) {
      try {
        const content = fs.readFileSync(configPath, "utf-8");

        // Check if AI is enabled
        if (!content.includes("ai:") && !content.includes("ai :")) {
          return {};
        }

        const enabledMatch = content.match(/ai\s*:\s*\{[^}]*enabled\s*:\s*(true|false)/s);
        if (enabledMatch && enabledMatch[1] === "false") return {};

        const modelMatch = content.match(/ai\s*:\s*\{[^}]*model\s*:\s*["']([^"']+)["']/s);
        const baseUrlMatch = content.match(/ai\s*:\s*\{[^}]*baseUrl\s*:\s*["']([^"']+)["']/s);
        // Match `apiKey: process.env.SOME_VAR` and resolve it at runtime
        const apiKeyMatch = content.match(/ai\s*:\s*\{[^}]*apiKey\s*:\s*process\.env\.(\w+)/s);
        const maxResultsMatch = content.match(/ai\s*:\s*\{[^}]*maxResults\s*:\s*(\d+)/s);
        const systemPromptMatch = content.match(
          /ai\s*:\s*\{[^}]*systemPrompt\s*:\s*["'`]([^"'`]+)["'`]/s,
        );

        return {
          enabled: true,
          model: modelMatch?.[1],
          baseUrl: baseUrlMatch?.[1],
          apiKey: apiKeyMatch?.[1] ? process.env[apiKeyMatch[1]] : undefined,
          maxResults: maxResultsMatch ? parseInt(maxResultsMatch[1], 10) : undefined,
          systemPrompt: systemPromptMatch?.[1],
        };
      } catch {
        // fall through
      }
    }
  }
  return {};
}

function stripMdx(raw: string): string {
  const { content } = matter(raw);
  return content
    .replace(/^(import|export)\s.*$/gm, "")
    .replace(/<[^>]+\/>/g, "")
    .replace(/<\/?[A-Z][^>]*>/g, "")
    .replace(/<\/?[a-z][^>]*>/g, "")
    .replace(/\[([^\]]+)\]\([^)]+\)/g, "$1")
    .replace(/!\[([^\]]*)\]\([^)]+\)/g, "$1")
    .replace(/^#{1,6}\s+/gm, "")
    .replace(/(\*{1,3}|_{1,3})(.*?)\1/g, "$2")
    .replace(/```[\s\S]*?```/g, "")
    .replace(/`([^`]+)`/g, "$1")
    .replace(/^>\s+/gm, "")
    .replace(/^[-*_]{3,}\s*$/gm, "")
    .replace(/\n{3,}/g, "\n\n")
    .trim();
}

function scanDocsDir(docsDir: string, entry: string): SearchIndex[] {
  const indexes: SearchIndex[] = [];

  function scan(dir: string, slugParts: string[]) {
    if (!fs.existsSync(dir)) return;

    const pagePath = path.join(dir, "page.mdx");
    if (fs.existsSync(pagePath)) {
      try {
        const raw = fs.readFileSync(pagePath, "utf-8");
        const { data } = matter(raw);
        const title =
          (data.title as string) ||
          slugParts[slugParts.length - 1]?.replace(/-/g, " ") ||
          "Documentation";
        const description = data.description as string | undefined;
        const content = stripMdx(raw);
        const url = slugParts.length === 0 ? `/${entry}` : `/${entry}/${slugParts.join("/")}`;

        indexes.push({ title, description, content, url });
      } catch {
        // skip unreadable files
      }
    }

    let entries: string[];
    try {
      entries = fs.readdirSync(dir);
    } catch {
      return;
    }

    for (const name of entries.sort()) {
      const full = path.join(dir, name);
      try {
        if (fs.statSync(full).isDirectory()) {
          scan(full, [...slugParts, name]);
        }
      } catch {
        // skip
      }
    }
  }

  scan(docsDir, []);
  return indexes;
}

// ─── AI Chat (RAG) ─────────────────────────────────────────────────

const DEFAULT_SYSTEM_PROMPT = `You are a helpful documentation assistant. Answer questions based on the provided documentation context. Be concise and accurate. If the answer is not in the context, say so honestly. Use markdown formatting for code examples and links.`;

interface ChatMessage {
  role: "user" | "assistant" | "system";
  content: string;
}

function resolveModelAndProvider(
  aiConfig: AIOptions,
  requestedModelId?: string,
): { model: string; baseUrl: string; apiKey: string | undefined } {
  const raw = aiConfig.model as AIModelConfig | string | undefined;

  // Find the model list (from nested or flat config)
  const modelList: AIModelEntry[] = (typeof raw === "object" && raw?.models) || [];

  // Resolve model id
  let modelId = requestedModelId;
  if (!modelId) {
    if (typeof raw === "string") modelId = raw;
    else if (typeof raw === "object") modelId = raw.defaultModel ?? raw.models?.[0]?.id;
    if (!modelId) modelId = "gpt-4o-mini";
  }

  // Find matching model entry to get its provider key
  const entry = modelList.find((m) => m.id === modelId);
  const providerKey = entry?.provider;

  // Resolve provider config
  const providerConfig = providerKey && aiConfig.providers?.[providerKey];

  const baseUrl = (
    (providerConfig && providerConfig.baseUrl) ||
    aiConfig.baseUrl ||
    "https://api.openai.com/v1"
  ).replace(/\/$/, "");

  const apiKey =
    (providerConfig && providerConfig.apiKey) || aiConfig.apiKey || process.env.OPENAI_API_KEY;

  return { model: modelId, baseUrl, apiKey };
}

async function handleAskAI(
  request: Request,
  indexes: SearchIndex[],
  searchServer: { search: (query: string) => Promise<unknown[]> },
  aiConfig: AIOptions,
): Promise<Response> {
  // ── Parse request ──────────────────────────────────────────────
  let body: { messages?: ChatMessage[]; model?: string };
  try {
    body = await request.json();
  } catch {
    return Response.json(
      { error: "Invalid JSON body. Expected { messages: [...] }" },
      { status: 400 },
    );
  }

  const messages = body.messages;
  if (!Array.isArray(messages) || messages.length === 0) {
    return Response.json(
      { error: "messages array is required and must not be empty." },
      { status: 400 },
    );
  }

  const lastUserMessage = [...messages].reverse().find((m) => m.role === "user");
  if (!lastUserMessage) {
    return Response.json({ error: "At least one user message is required." }, { status: 400 });
  }

  // ── Search for relevant docs (RAG retrieval) ───────────────────
  const maxResults = aiConfig.maxResults ?? 5;
  const query = lastUserMessage.content;

  // Use the in-memory index for fast context retrieval
  const scored = indexes
    .map((doc) => {
      const q = query.toLowerCase();
      const titleMatch = doc.title.toLowerCase().includes(q) ? 10 : 0;
      const contentWords = q.split(/\s+/);
      const contentMatch = contentWords.reduce((score, word) => {
        return score + (doc.content.toLowerCase().includes(word) ? 1 : 0);
      }, 0);
      return { ...doc, score: titleMatch + contentMatch };
    })
    .filter((d) => d.score > 0)
    .sort((a, b) => b.score - a.score)
    .slice(0, maxResults);

  // Build context from relevant docs
  const contextParts = scored.map(
    (doc) =>
      `## ${doc.title}\nURL: ${doc.url}\n${doc.description ? `Description: ${doc.description}\n` : ""}\n${doc.content}`,
  );
  const context = contextParts.join("\n\n---\n\n");

  // ── Build LLM messages ─────────────────────────────────────────
  const systemPrompt = aiConfig.systemPrompt ?? DEFAULT_SYSTEM_PROMPT;
  const systemMessage: ChatMessage = {
    role: "system",
    content: context
      ? `${systemPrompt}\n\n---\n\nDocumentation context:\n\n${context}`
      : systemPrompt,
  };

  const llmMessages: ChatMessage[] = [
    systemMessage,
    ...messages.filter((m) => m.role !== "system"),
  ];

  // ── Resolve model + provider ────────────────────────────────────
  const requestedModel =
    typeof body.model === "string" && body.model.trim().length > 0 ? body.model.trim() : undefined;

  const resolved = resolveModelAndProvider(aiConfig, requestedModel);

  if (!resolved.apiKey) {
    return Response.json(
      {
        error: `AI is enabled but no API key was found. Either set apiKey in your docs.config ai section, configure a provider, or add OPENAI_API_KEY to your .env.local file.`,
      },
      { status: 500 },
    );
  }

  const llmResponse = await fetch(`${resolved.baseUrl}/chat/completions`, {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      Authorization: `Bearer ${resolved.apiKey}`,
    },
    body: JSON.stringify({
      model: resolved.model,
      stream: true,
      messages: llmMessages,
    }),
  });

  if (!llmResponse.ok) {
    const errText = await llmResponse.text().catch(() => "Unknown error");
    return Response.json(
      { error: `LLM API error (${llmResponse.status}): ${errText}` },
      { status: 502 },
    );
  }

  // Proxy the SSE stream directly to the client
  return new Response(llmResponse.body, {
    headers: {
      "Content-Type": "text/event-stream",
      "Cache-Control": "no-cache",
      Connection: "keep-alive",
    },
  });
}

// ─── llms.txt generation ────────────────────────────────────────────

interface LlmsTxtOptions {
  siteTitle?: string;
  siteDescription?: string;
  baseUrl?: string;
}

function readLlmsTxtConfig(root: string): LlmsTxtOptions & { enabled: boolean } {
  for (const ext of FILE_EXTS) {
    const configPath = path.join(root, `docs.config.${ext}`);
    if (fs.existsSync(configPath)) {
      try {
        const content = fs.readFileSync(configPath, "utf-8");

        if (!content.includes("llmsTxt")) return { enabled: false };

        const isBoolTrue = /llmsTxt\s*:\s*true/.test(content);
        if (isBoolTrue) return { enabled: true };

        const enabledMatch = content.match(/llmsTxt\s*:\s*\{[^}]*enabled\s*:\s*(true|false)/s);
        if (enabledMatch && enabledMatch[1] === "false") return { enabled: false };

        const baseUrlMatch = content.match(/llmsTxt\s*:\s*\{[^}]*baseUrl\s*:\s*["']([^"']+)["']/s);
        const siteTitleMatch = content.match(
          /llmsTxt\s*:\s*\{[^}]*siteTitle\s*:\s*["']([^"']+)["']/s,
        );
        const siteDescMatch = content.match(
          /llmsTxt\s*:\s*\{[^}]*siteDescription\s*:\s*["']([^"']+)["']/s,
        );

        const navTitleMatch = content.match(/nav\s*:\s*\{[^}]*title\s*:\s*["']([^"']+)["']/s);

        return {
          enabled: true,
          baseUrl: baseUrlMatch?.[1],
          siteTitle: siteTitleMatch?.[1] ?? navTitleMatch?.[1],
          siteDescription: siteDescMatch?.[1],
        };
      } catch {
        // fall through
      }
    }
  }
  return { enabled: false };
}

function generateLlmsTxt(
  indexes: SearchIndex[],
  options: LlmsTxtOptions,
): { llmsTxt: string; llmsFullTxt: string } {
  const { siteTitle = "Documentation", siteDescription, baseUrl = "" } = options;

  let llmsTxt = `# ${siteTitle}\n\n`;
  if (siteDescription) llmsTxt += `> ${siteDescription}\n\n`;
  llmsTxt += `## Pages\n\n`;
  for (const page of indexes) {
    llmsTxt += `- [${page.title}](${baseUrl}${page.url})`;
    if (page.description) llmsTxt += `: ${page.description}`;
    llmsTxt += `\n`;
  }

  let llmsFullTxt = `# ${siteTitle}\n\n`;
  if (siteDescription) llmsFullTxt += `> ${siteDescription}\n\n`;
  for (const page of indexes) {
    llmsFullTxt += `## ${page.title}\n\n`;
    llmsFullTxt += `URL: ${baseUrl}${page.url}\n\n`;
    if (page.description) llmsFullTxt += `${page.description}\n\n`;
    llmsFullTxt += `${page.content}\n\n---\n\n`;
  }

  return { llmsTxt, llmsFullTxt };
}

// ─── createDocsAPI ──────────────────────────────────────────────────

/**
 * Create a unified docs API route handler.
 *
 * Returns `{ GET, POST }` for use in a Next.js route handler:
 * - **GET  ?query=…**        → full-text search
 * - **GET  ?format=llms**     → llms.txt (concise page listing)
 * - **GET  ?format=llms-full** → llms-full.txt (full page content)
 * - **POST**                  → AI-powered chat with RAG
 *
 * @example
 * ```ts
 * // app/api/docs/route.ts (auto-generated by withDocs)
 * import { createDocsAPI } from "@farming-labs/theme/api";
 * export const { GET, POST } = createDocsAPI();
 * export const revalidate = false;
 * ```
 *
 * @param options - Optional overrides (entry, language, ai config)
 */
export function createDocsAPI(options?: DocsAPIOptions) {
  const root = process.cwd();
  const entry = options?.entry ?? readEntry(root);
  const docsDir = path.join(root, "app", entry);
  const language = options?.language ?? "english";

  // Read AI config from docs.config if not explicitly provided
  const aiConfig: AIOptions = options?.ai ?? readAIConfig(root);

  // Read llms.txt config
  const llmsConfig = readLlmsTxtConfig(root);

  // Build search indexes (shared between search, AI, and llms.txt)
  const indexes = scanDocsDir(docsDir, entry);

  // Pre-generate llms.txt content (cached in memory)
  let _llmsCache: { llmsTxt: string; llmsFullTxt: string } | null = null;
  function getLlmsContent() {
    if (!_llmsCache) {
      _llmsCache = generateLlmsTxt(indexes, {
        siteTitle: llmsConfig.siteTitle ?? "Documentation",
        siteDescription: llmsConfig.siteDescription,
        baseUrl: llmsConfig.baseUrl ?? "",
      });
    }
    return _llmsCache;
  }

  // Create the fumadocs-core search API (provides GET handler)
  const searchAPI = createSearchAPI(
    "simple" as const,
    {
      language,
      indexes,
    } as any,
  );

  return {
    /**
     * GET handler — search, llms.txt, or llms-full.txt depending on query params.
     */
    GET(request: Request) {
      const url = new URL(request.url);
      const format = url.searchParams.get("format");

      if (format === "llms") {
        return new Response(getLlmsContent().llmsTxt, {
          headers: {
            "Content-Type": "text/plain; charset=utf-8",
            "Cache-Control": "public, max-age=3600",
          },
        });
      }

      if (format === "llms-full") {
        return new Response(getLlmsContent().llmsFullTxt, {
          headers: {
            "Content-Type": "text/plain; charset=utf-8",
            "Cache-Control": "public, max-age=3600",
          },
        });
      }

      return searchAPI.GET(request);
    },

    /**
     * POST handler — AI chat with RAG.
     * Body: `{ messages: [{ role: "user", content: "How do I …?" }] }`
     * Response: SSE stream of chat completion chunks.
     */
    async POST(request: Request): Promise<Response> {
      if (!aiConfig.enabled) {
        return Response.json(
          {
            error:
              "AI is not enabled. Set `ai: { enabled: true }` in your docs.config to enable it.",
          },
          { status: 404 },
        );
      }

      return handleAskAI(request, indexes, searchAPI, aiConfig);
    },
  };
}
